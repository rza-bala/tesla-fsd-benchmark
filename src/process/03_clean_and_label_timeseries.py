#!/usr/bin/env python3
"""
Cleans and labels downsampled time series:
- Drops weak signals (>10% nulls or <2 unique if not enum)
- Applies enum mapping using enum_maps.json
- Saves filtered Parquet files to data/processed/
- Generates signal cleaning summary report
"""
import sys
import pandas as pd
import json
import logging
from datetime import datetime
from pathlib import Path


# â”€â”€â”€ Force src/ to be discoverable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.utils.paths import DOWNSAMPLED_DIR, PROCESSED_DIR, REGISTRY_DIR, ENUM_MAPS_PATH


PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_NULL_FRAC = 0.10
MIN_UNIQUE_NON_ENUM = 2

# â”€â”€â”€ Logging Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s"
)

# â”€â”€â”€ Load Enum Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(ENUM_MAPS_PATH) as f:
    enum_map = json.load(f)
enum_signals = set(enum_map.keys())

# â”€â”€â”€ Signal Filtering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def filter_signals(df: pd.DataFrame, record: dict) -> pd.DataFrame:
    keep = ["time"]
    for col in df.columns:
        if col == "time":
            continue
        null_frac = df[col].isna().mean()
        unique_vals = df[col].nunique(dropna=True)
        is_enum = col in enum_signals

        if null_frac > MAX_NULL_FRAC:
            record["dropped"].append((col, "high nulls"))
        elif not is_enum and unique_vals < MIN_UNIQUE_NON_ENUM:
            record["dropped"].append((col, "low uniqueness"))
        else:
            keep.append(col)
            record["kept"].append(col)

    for col, reason in record["dropped"]:
        logging.warning(f"âš ï¸ Dropped: {col} â†’ {reason}")
    logging.info(f"ğŸ§¹ Dropped {len(record['dropped'])} signals, kept {len(record['kept']) + 1}")
    return df[keep]

# â”€â”€â”€ Enum Labeling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apply_enum_labels(df: pd.DataFrame, enum_map: dict) -> pd.DataFrame:
    mapped = 0

    for col in df.columns:
        if col in enum_map:
            try:
                if not pd.api.types.is_numeric_dtype(df[col]):
                    raise TypeError("non-numeric")

                # Convert safely to int where possible
                df[col] = (
                    df[col]
                    .round()
                    .astype("Int64")  # nullable int for safety
                    .astype(str)
                    .map(enum_map[col])
                    .fillna(df[col])
                )
                mapped += 1
            except Exception as e:
                logging.warning(f"âš ï¸ Enum skipped for {col}: {e}")

    logging.info(f"ğŸ§  Enum signals mapped: {mapped}")
    return df


# â”€â”€â”€ Main Processing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
summary = []

for pq_file in sorted(DOWNSAMPLED_DIR.glob("*.parquet")):
    logging.info(f"ğŸ§¼ Cleaning & labeling: {pq_file.name}")
    try:
        df = pd.read_parquet(pq_file)
        record = {
            "file": pq_file.name,
            "dropped": [],
            "kept": [],
            "enum_mapped": [],
            "enum_failed": [],
        }
        df = filter_signals(df, record)
        df = apply_enum_labels(df, enum_map, record)

        out_path = PROCESSED_DIR / pq_file.name.replace(".parquet", "_filtered.parquet")
        df.to_parquet(out_path, index=False)
        logging.info(f"âœ… Saved filtered file: {out_path}")
        summary.append(record)

    except Exception as e:
        logging.error(f"âŒ Failed to process {pq_file.name}: {e}")

