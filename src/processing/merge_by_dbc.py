#!/usr/bin/env python3
"""
Merges all *_downsampled.parquet files grouped by DBC tag.
Outputs: data/processed/{dbc_tag}.parquet
"""

import pandas as pd
from pathlib import Path
import logging

# â”€â”€â”€ Path Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
INTERIM_DIR = PROJECT_ROOT / "data" / "interim"
PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

# â”€â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s â€” %(levelname)s â€” %(message)s",
    handlers=[logging.StreamHandler()]
)

# â”€â”€â”€ Group Files by DBC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
grouped = {}
for file in sorted(INTERIM_DIR.glob("*_downsampled.parquet")):
    try:
        dbc_tag = file.stem.split("_", maxsplit=1)[1].replace("_downsampled", "")
        grouped.setdefault(dbc_tag, []).append(file)
    except IndexError:
        logging.warning(f"âŒ Unexpected filename format: {file.name}")

# â”€â”€â”€ Merge by Concatenation (stacking) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for dbc_tag, files in grouped.items():
    logging.info(f"ğŸ” Merging {len(files)} files for DBC: {dbc_tag}")
    dfs = []

    for f in files:
        df = pd.read_parquet(f)
        if "time" not in df.columns:
            logging.warning(f"âš ï¸ Skipping {f.name} (no 'time' column)")
            continue
        dfs.append(df)

    if dfs:
        merged_df = pd.concat(dfs, ignore_index=True)
        merged_df["time"] = pd.to_datetime(merged_df["time"], utc=True)
        merged_df = merged_df.sort_values("time")
        out_path = PROCESSED_DIR / f"{dbc_tag}.parquet"
        merged_df.to_parquet(out_path, index=False)
        logging.info(f"âœ… Saved merged file: {out_path}")
    else:
        logging.warning(f"âš ï¸ No valid files to merge for {dbc_tag}")
